{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e9961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddd7badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import RFE, SelectKBest, chi2, f_classif, SelectFromModel\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3888871e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "709719bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, X, y):\n",
    "    \n",
    "    trained_model = model.fit(X, y)\n",
    "    \n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3faac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_reg(model, X, y):\n",
    "    \n",
    "    y_pred = model.predict(y)\n",
    "    scoring = model.score(X, y)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    \n",
    "    return scoring, mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5129fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_clf(trained_model, X, y):\n",
    "    \n",
    "    y_pred = trained_model.predict(X)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    roc = roc_auc_score(y, y_pred)\n",
    "    prec = precision_score(y, y_pred)\n",
    "    rec = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    \n",
    "    return acc, roc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32c4910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_get_metrics_reg(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    trained_model = fit_model(model, X_train, y_train)\n",
    "    scoring, mse, mae = calculate_metrics_reg(trained_model, X_test, y_test)\n",
    "    \n",
    "    return scoring, mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae618b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_get_metrics_clf(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    trained_model = fit_model(model, X_train, y_train)\n",
    "    acc, roc, prec, rec, f1 = calculate_metrics_clf(trained_model, X_test, y_test)\n",
    "    \n",
    "    return acc, roc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fcf8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_reg_model_on_features(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    scoring, mse, mae = train_get_metrics_reg(model, X_train, y_train, X_test, y_test)\n",
    "    display_df = pd.DataFrame([[scoring, mse, mae, X_train.shape[1]]],\n",
    "                              columns = ['Accuracy', 'MSE', 'MAE', 'Feature Count'])\n",
    "    \n",
    "    return display_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e68257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_clf_model_on_features(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    acc, roc, prec, rec, f1 = train_get_metrics_clf(model, X_train, y_train, X_test, y_test)\n",
    "    display_df = pd.DataFrame([[acc, roc, prec, rec, f1, X_train.shape[1]]],\n",
    "                              columns = ['Accuracy', 'ROC', 'Precision', 'Recall', 'F1-score', 'Feature Count'])\n",
    "    \n",
    "    return display_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06b65fd",
   "metadata": {},
   "source": [
    "# Filter Method\n",
    "\n",
    "\n",
    "###  Univariate Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98926b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_selection(X_train, y_train, model_type, k):\n",
    "    \n",
    "    selector = SelectKBest(model_type, k = k)\n",
    "    X_new = selector.fit_transform(X_train, y_train)\n",
    "    feature_idx = selector.get_support()\n",
    "    features_name = X_train.columns[feature_idx]\n",
    "    \n",
    "    return features_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599ce130",
   "metadata": {},
   "source": [
    "# Wrapper Method\n",
    "\n",
    "\n",
    "### Recursive feature elimination - RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d31f3278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rfe(model, X_train, y_train, n):\n",
    "    \n",
    "    rfe = RFE(model, n_features_to_select = n)\n",
    "    rfe = rfe.fit(X_train, y_train)\n",
    "    features_name = X_train.columns[rfe.get_support()]\n",
    "    \n",
    "    return features_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfc4599",
   "metadata": {},
   "source": [
    "# Embedded Method\n",
    "\n",
    "\n",
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abfe7593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_based_feature_importances(trained_model, X_train, plot = True):\n",
    "    \n",
    "    imp_df = pd.DataFrame({'Feature': X_train.columns,\n",
    "                          'Importances': trained_model.feature_importances_}.sort_values('Importances', \n",
    "                                                                                         ascending = False))\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sb.barplot(data = rf_imp, x = 'Importances', y = 'Features')\n",
    "        plt.title(\"Feature Importances (train set)\")\n",
    "        plt.show()\n",
    "        \n",
    "    return imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0053bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_importance_features(trained_model, X, y, n, st, plot = True):\n",
    "    \n",
    "    result = permutation_importance(trained_model, X, y, n_repeats = 10, random_state = st)\n",
    "    imp_order = result.importances_mean.argsort()\n",
    "    ft_importance = pd.DataFrame(result.importances[imp_order].T,\n",
    "                                 columns = X.columns[imp_order])\n",
    "    imp_df = ft_importance[ft_importance.columns[::-1]]\n",
    "    if plot:\n",
    "        plt.figure(figsize = (10, 6))\n",
    "        ax = sb.boxplot(data = imp_df, orient = 'h')\n",
    "        ax.set_title(\"Permutation Importances\")\n",
    "        ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "        ax.set_xlabel(\"Decrease in accuracy score\")\n",
    "        ax.figure.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    return imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5425d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_l1_regularization(model, X_train, y_train):\n",
    "    \n",
    "    # Select L1 regulated features from LinearSVC output \n",
    "    selection = SelectFromModel(model)\n",
    "    selection.fit(X_train, y_train)\n",
    "\n",
    "    feature_names = X_train.columns[(selection.get_support())]\n",
    "    \n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a95ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
